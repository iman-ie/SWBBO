import botorch
import torch
# from kdesbo.utils import generate_initial_data, sample_kde, qmc_sample_kde, update_edf, get_kernel_matrix
from .utils import generate_initial_data, sample_kde, update_edf, get_kernel_matrix, sample_context_for_L_calculation
from botorch.models.gp_regression import SingleTaskGP
import math
from botorch.optim.optimize import optimize_acqf
from .acquisition import KDE_UCB, Stable_UCB, MMD_UCB, MMD_Minimax_Approx_UCB, KDE_DRBO_UCB, Wasserstein_UCB,SinkhornBarycenter_UCB, SinkhornBarycenter_POT_UCB
from .visualization import plot_barycenter_with_distributions, plot_2d_barycenter_with_distributions
from .lp_barycenter import LPBarycenterComputer  # Import the LP barycenter class
from .barycenter_comparison import plot_barycenter_comparison, compute_barycenter_distance_metrics
from botorch.fit import fit_gpytorch_mll
from gpytorch.mlls.sum_marginal_log_likelihood import ExactMarginalLogLikelihood
from botorch.acquisition.analytic import UpperConfidenceBound, ExpectedImprovement
from botorch.generation.gen import gen_candidates_scipy, TGenCandidates, gen_candidates_torch


class BOTorchOptimizer:
    def __init__(self, problem, init_size=10, running_rounds=200, device=torch.device('cpu'), beta=1.5):
        self.problem = problem
        self.init_size = init_size
        self.running_rounds = running_rounds
        self.train_X, self.train_Y, self.contexts = generate_initial_data(self.problem, self.init_size)
        self.stochastic_Y = None
        self.beta = beta
        self.best_Y = torch.max(self.train_Y)
        self.best_stochastic_Y = None
        self.cumulative_reward = None   
        self.cumulative_stochastic_regret = None
        self.cumulative_stochastic_reward = None
        self.device=device
        self.bounds = self.problem.bounds.to(self.device)

    def get_model(self, context=False):
        X_contexts = self.train_X
        if context:
            X_contexts = torch.cat((X_contexts, self.contexts), dim=-1)
        mean = self.train_Y.mean()
        sigma = self.train_Y.std()
        Y = (self.train_Y-mean)/sigma
        model = SingleTaskGP(X_contexts, Y.reshape(-1, 1)).to(device=self.device)
        mll = ExactMarginalLogLikelihood(model.likelihood, model)
        fit_gpytorch_mll(mll)
        return model

    def evaluate_new_candidates(self, candidates, i):
        candidates = candidates.cpu()
        new_Y, new_contexts = self.problem(candidates) # this calls the evaluate_true function via the forward function
        self.train_X = torch.cat((self.train_X, candidates), dim=0)
        self.train_Y = torch.cat((self.train_Y, new_Y), dim=0)
        self.contexts = torch.cat((self.contexts, new_contexts), dim=0)
        if self.best_Y < new_Y:
            self.best_Y = new_Y
        print(f"At running_rounds {i}, the best instantaneous value is :{self.best_Y}")
        if self.cumulative_reward is None:
            self.cumulative_reward = torch.sum(self.train_Y.reshape(-1,))
        else:
            self.cumulative_reward += new_Y[0]
        next_stochastic_Y = None
        if self.problem.can_calculate_stochastic:
            next_stochastic_Y = None
            if self.stochastic_Y is None:
                self.stochastic_Y = torch.zeros_like(self.train_Y)
                for i, x in enumerate(self.train_X):
                    next_stochastic_Y = self.problem.evaluate_stochastic(x.reshape(1, -1))
                    self.stochastic_Y[i] = next_stochastic_Y
                self.best_stochastic_Y = torch.max(self.stochastic_Y)
                self.cumulative_stochastic_regret = torch.sum(self.problem.max_stochastic -self.stochastic_Y)
                self.cumulative_stochastic_reward = torch.sum(self.stochastic_Y)
            else:
                next_stochastic_Y = self.problem.evaluate_stochastic(candidates.reshape(1, -1))
                self.stochastic_Y = torch.cat((self.stochastic_Y, next_stochastic_Y.reshape(1, )))
                if self.best_stochastic_Y < next_stochastic_Y:
                    self.best_stochastic_Y = next_stochastic_Y
                self.cumulative_stochastic_regret += self.problem.max_stochastic-next_stochastic_Y
                self.cumulative_stochastic_reward += next_stochastic_Y
            # wandb.log({f"Best_Stochastic_Value": self.best_stochastic_Y, f"Best_Value": self.best_Y,
            #            f"Cumulative Stochastic Reward": self.cumulative_stochastic_reward,
            #            f"Cumulative Stochastic Regret": self.cumulative_stochastic_regret,
            #            f"Cumulative Reward":self.cumulative_reward})
        # else:
        #     wandb.log({f"Best_Value": self.best_Y, f"Cumulative Reward":self.cumulative_reward})
        print(f"At running_rounds {i}, the best robust value is :{self.best_stochastic_Y}")
        print(f"Candidate :{candidates}, new_value :{new_Y}, next_exp_value: {next_stochastic_Y}")

    def run_opt(self):
        raise NotImplementedError


class SinkhornWassersteinUCB1_Optimizer(BOTorchOptimizer):
    """Improved Sinkhorn Barycenter Wasserstein UCB Optimizer."""

    def __init__(self, problem, init_size=10, running_rounds=200, device=torch.device('cpu'), beta=1.5):
        super().__init__(problem, init_size, running_rounds, device=device, beta=beta)
        self.contexts_for_L = sample_context_for_L_calculation(
            100, self.problem.bounds[:, -self.problem.contexts_dim:]
        )
        # Better parameters for more significant barycenter effect
        self.reg = 0.001  #  regularization for better smoothing
        self.num_sinkhorn_iters = 100  #  iterations for convergence

    def run_opt(self):
        # Add comparison tracking (Variables used only when the code optionally compares two acquisitions)
        total_context_distance = 0
        comparison_rounds = 0

        for i in range(self.init_size, self.running_rounds): #Loop index i counts total evaluations, so iâˆ’init_size is the iteration number.

            model = self.get_model(context=True)

            # Create both acquisition functions for comparison
            sinkhorn_acq = SinkhornBarycenter_POT_UCB(
                model=model,
                beta=self.beta,
                kde_samples=self.contexts, # empirical context bank
                contexts_samples_for_L=self.contexts_for_L,
                radius=0.3 / math.sqrt(i),
                reg=self.reg,
                num_sinkhorn_iters=self.num_sinkhorn_iters,
            )

            if  i == self.running_rounds - 1:  # Check every 20 iterations and at the end
                metrics = sinkhorn_acq.compute_barycenter_metrics()
                print(f"Round {i}: Barycenter Quality - BW22-UVP: {metrics['BW22-UVP']:.4f}%")
                if metrics['L2-UVP'] is not None:
                    print(f"Round {i}: L2-UVP: {metrics['L2-UVP']:.4f}%")
                else:
                    print(f"Round {i}: L2-UVP: Not available")
            # Store the final acquisition function
            last_sinkhorn_acq = sinkhorn_acq


            # Optional: Create original for comparison every few rounds
            if i % 10 == 0:
                from .acquisition import Wasserstein_UCB

                original_acq = Wasserstein_UCB(
                    model=model,
                    beta=self.beta,
                    kde_samples=self.contexts,
                    contexts_samples_for_L=self.contexts_for_L,
                    radius=0.3 / math.sqrt(i),
                )

                # Test on a few random points to see difference (Draws five random test points and prints the mean absolute difference
                # between the two acquisition scores.)
                test_X = torch.rand(5, self.problem.dim - self.problem.contexts_dim, device=self.device)
                test_X = test_X * (self.bounds[1, :-self.problem.contexts_dim] - self.bounds[0,
                                                                                 :-self.problem.contexts_dim]) + self.bounds[
                                                                                                                 0,
                                                                                                                 :-self.problem.contexts_dim]

                sinkhorn_vals = sinkhorn_acq(test_X.unsqueeze(1))
                original_vals = original_acq(test_X.unsqueeze(1))

                diff = torch.mean(torch.abs(sinkhorn_vals - original_vals)).item()
                print(f"Round {i}: Average acquisition difference: {diff:.6f}")

                if diff > 1e-6:
                    print(f"  -> Significant difference detected!")
                else:
                    print(f"  -> Minimal difference - may need parameter tuning")

            candidates, _ = optimize_acqf(
                acq_function=sinkhorn_acq,
                bounds=self.bounds[:, :-self.problem.contexts_dim],
                q=1,
                num_restarts=10,
                raw_samples=1024,
                gen_candidates=gen_candidates_torch,  #Each restart runs gradient-based optimisation (L-BFGS) inside gen_candidates_torch
                options={"batch_limit": 512},
            )

            self.evaluate_new_candidates(candidates.detach(), i)


        # Plot only at the end of all iterations

        if last_sinkhorn_acq is not None:
            print(f"\n=== Final Barycenter Visualization ===")
            plot_barycenter_with_distributions(
                sinkhorn_acq,
                save_path="final_barycenter.png",
                title=f"Final Barycenter after {self.running_rounds} iterations"
            )

            # Compute LP barycenter for comparison
            print(f"\n=== Computing LP Barycenter for Comparison ===")
            try:
                lp_barycenter_computer = LPBarycenterComputer(
                    kde_samples=self.contexts,
                    num_distributions=min(10, len(self.contexts)),  # Same as Sinkhorn
                    distribution_method='random',
                    barycenter_support_size=50
                )

                print(
                    f"LP barycenter computed successfully with {len(lp_barycenter_computer.get_barycenter_samples())} samples")

                # Plot comparison between Sinkhorn and LP barycenters
                print(f"\n=== Plotting Barycenter Comparison ===")
                plot_barycenter_comparison(
                    sinkhorn_acq=last_sinkhorn_acq,
                    lp_barycenter_computer=lp_barycenter_computer,
                    save_path="sinkhorn_vs_lp_barycenter_comparison.png",
                    title=f"Sinkhorn vs LP Barycenter Comparison (Round {self.running_rounds})"
                )

                # Compute and display distance metrics
                distance_metrics = compute_barycenter_distance_metrics(
                    last_sinkhorn_acq.barycenter_samples,
                    lp_barycenter_computer.get_barycenter_samples()
                )

                # Optional: Save comparison data for later analysis
                comparison_data = {
                    'sinkhorn_barycenter': last_sinkhorn_acq.barycenter_samples.detach().cpu().numpy(),
                    'lp_barycenter': lp_barycenter_computer.get_barycenter_samples().detach().cpu().numpy(),
                    'original_contexts': self.contexts.detach().cpu().numpy(),
                    'distance_metrics': distance_metrics,
                    'sinkhorn_params': {
                        'reg': self.reg,
                        'num_sinkhorn_iters': self.num_sinkhorn_iters,
                        'num_distributions': min(10, len(self.contexts))
                    }
                }

                # Save to file (optional)
                import pickle
                with open('barycenter_comparison_data.pkl', 'wb') as f:
                    pickle.dump(comparison_data, f)
                print("Comparison data saved to 'barycenter_comparison_data.pkl'")

            except Exception as e:
                print(f"Error in LP barycenter computation or comparison: {e}")
                print("Continuing without LP barycenter comparison...")




        return (
            self.train_X,
            self.train_Y,
            self.contexts,
            self.cumulative_stochastic_regret,
            self.best_Y,
            self.stochastic_Y,
        )
